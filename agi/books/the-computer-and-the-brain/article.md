---
title: John von Neumann - The Computer and the Brain - 1958
created: 2015-12-31
taxonomy:
  tag: [artificial-general-intelligence]
  status: finished
---

## Context

## Learned in this study

## Things to explore
* What is referred as "sense coincidences" and "sense anticoincidences"?
* Within the brain, at which point do neurons start firing up? (basically, what is the big bang of a brain?)
* Binary neural networks (no weights, only active/inactive and gate-like, and/or)
* What if synapses are really the basic active organ and not the neuron's body?
* Since ALU are logic units (more complex), everything in a computer is basically governed by logic?
* What is entailed by the idea that instead of perceiving through time, we perceive through frequency?

# Overview
* 4 basic arithmetic operations: addition, subtraction, multiplication, division
* A branching/transfer operation

# Notes
* (p1) Two classes of machines: analog and digital.
* (p2) Organs/Components are required to manipulate the numbers according to a plan (to compute).
These organs must be able to perform the 4 basic operations of mathematics (addition, subtraction, multiplication and division).
* (p11) The intention of the user is impressed on the machine (called "setting up"). In both the mechanical and electrical machines, the intention was always a fixed setting for the entire duration of a problem.
* (p14) Due to the "only one organ for each operation" principle, there is a need for a special organ/component that will store numbers. It is referred as the "memory register", where the number of register in a memory is the capacity of that memory.
* (p29-30) Organs which perform the basic logical actions: sense coincidences, combine stimuli and sense anticoincidences.
* (p31) The arithmetic organ consists of about 300 to 2000 active organs (logic gates?).
Certain aggregates of active organs are used to perform some memory function. These are composed of about 200 to 2000 active organs.
Memory organs (registers) require about 300 to 2000 active organs.
Memory may represent as much as 50% of the entire machine.
* (p40) A nerve cell consists of a *body* from which originate, directly or indirectly, one or more branches. Such a branch is called an *axon* of the cell.
The nerve impulse is a continuous change, propagated - usually at fixed speed, which may, however, be a function of the nerve cell involved - along each axon.
* (p43) The signal "emitted" by a neuron can be compared to a binary signal (0/1), and thus is digital in nature.
* (p44) A neuron will conditionally emit a secondary pulse (output) in response to certain combinations and synchronisms of primary pulses (input).
* (p45) Stimulation-receptors are called *dendrites*. The normal stimulation, when it comes from another pulse (or pulses) emanates from a special ending of the axon (or axons) that propagated the pulse in question. Their ending is called a *synapse*.
* (p51) The same factors show that the natural componentry favors automata with more, but slower, organs, while the artificial one favors the reverse arrangement of fewer, but faster, organs. Hence it is to be expected that an efficiently organized large natural automation (like the human nervous system) will tend to pick up as many logical (or informational) items as possible simultaneously, and process them simultaneously, while an efficiently organized large artificial automation (like a large modern computing machine) will be more likely to do things successively — one thing at a time, or at any rate not so many things at a time. That is, large and efficient natural automata are likely to be highly parallel, while large and efficient artificial automata will tend to be less so, and rather to be serial.
* (p52) Due to their different nature (sequential vs parallel), the artificial automata may have new memory requirements to temporarily store the results that would be computed by the natural * (parallel) automata. Similarly, a serial scheme may not be convertible into a parallel scheme.
* (p64) Approximately 14 x 10^10 bits per seconds of neuron activity. Over a lifetime of 60 years (2 x 10^9), that is 14 x 10^10 x 2 x 10^9 = 2.8 x 10^20 bits.
* (p65) It's been suggested that the physical embodiment of memory is through the neuron threshold. In other words, the more frequently a neuron is used, the lower its threshold may become (easier to stimulate). (threshold memory)
Another form of embodiment is through the distribution of conduting axons, varying with respect to time. (axon memory)
Another form of memory, which is obviously present, is the genetic part of the body: the chromosomes and their constituent genes are clearly memory elements which by their state affect, and to a certain extent determine, the functioning of the entire system. (genetic memory)
* (p67) In an artificial computing machine, memory is represented at its roots as flip-flops.
* (p68) Since we know that large store memory is not an active organ/component, we're forced to believe that some similar large capacity memory is also available/used with the nervous system.
* (p72) Short codes: A system of instructions which makes one machine imitate the behavior of another.
* (p80) Instead of the precise systems of markers where the position—and presence or absence—of every marker counts decisively in determining the meaning of the message, we have here a system of notations in which the meaning is conveyed by the statistical properties of the message.
Favors logic over arithmetic.

# See also

# References
* Neumann, John. The Computer & the Brain. 3rd ed. New Haven: Yale University Press, 2012.
